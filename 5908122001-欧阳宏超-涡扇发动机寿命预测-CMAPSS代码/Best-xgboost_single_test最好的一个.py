"""
åŸºäºenhanced_hybrid_rul_systemæ¡†æ¶çš„XGBoostå•ç‹¬æµ‹è¯•
ä½¿ç”¨è®­ç»ƒå¾—åˆ°çš„æœ€ä½³å‚æ•°è¿›è¡Œæ€§èƒ½è¯„ä¼°
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
import warnings
from pathlib import Path
import pickle
import json

# å¯¼å…¥å¿…è¦çš„åº“
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class XGBoostSingleTester:
    """XGBoostå•ç‹¬æµ‹è¯•å™¨"""
    
    def __init__(self, dataset_name='FD001'):
        self.dataset_name = dataset_name
        self.window_size = 30
        self.max_rul = 125
        self.scaler = StandardScaler()
        
        # å­˜å‚¨ç»“æœ
        self.model = None
        self.results = {}
        self.training_time = 0
        self.prediction_time = 0
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(f'xgboost_test_results_{dataset_name}')
        self.output_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ¯ XGBoostå•ç‹¬æµ‹è¯•å™¨åˆå§‹åŒ–")
        print(f"ğŸ“Š æ•°æ®é›†: {dataset_name}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def load_and_preprocess_data(self):
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ® - å¤ç”¨enhanced_hybrid_rul_systemçš„é€»è¾‘"""
        print(f"ğŸ“Š åŠ è½½æ•°æ®é›† {self.dataset_name}...")
        
        # å®šä¹‰åˆ—å
        cols = ['unit_number', 'time_cycles', 'op_setting_1', 'op_setting_2', 'op_setting_3'] + \
               [f'sensor_{i}' for i in range(1, 22)]
        
        # åŠ è½½æ•°æ®
        self.train_df = pd.read_csv(f'train_{self.dataset_name}.txt', sep=r'\s+', header=None, names=cols)
        self.test_df = pd.read_csv(f'test_{self.dataset_name}.txt', sep=r'\s+', header=None, names=cols)
        self.rul_df = pd.read_csv(f'RUL_{self.dataset_name}.txt', sep=r'\s+', header=None, names=['RUL'])
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {self.train_df.shape}")
        print(f"   æµ‹è¯•é›†: {self.test_df.shape}")
        print(f"   RULæ ‡ç­¾: {self.rul_df.shape}")
        
        # è®¡ç®—è®­ç»ƒé›†RUL
        def calculate_rul(group):
            group = group.copy()
            group['RUL'] = group['time_cycles'].max() - group['time_cycles']
            group['RUL'] = group['RUL'].clip(upper=self.max_rul)
            return group
        
        self.train_df = self.train_df.groupby('unit_number').apply(calculate_rul).reset_index(drop=True)
        
        # ç‰¹å¾é€‰æ‹© - é€‰æ‹©æœ‰å˜åŒ–çš„ä¼ æ„Ÿå™¨
        sensor_cols = [col for col in self.train_df.columns if 'sensor' in col]
        std_values = self.train_df[sensor_cols].std()
        useful_sensors = std_values[std_values > 0.01].index.tolist()
        
        self.feature_cols = ['op_setting_1', 'op_setting_2', 'op_setting_3'] + useful_sensors
        
        print(f"ğŸ”§ ç‰¹å¾é€‰æ‹©å®Œæˆï¼Œä½¿ç”¨ {len(self.feature_cols)} ä¸ªç‰¹å¾")
        print(f"   æœ‰ç”¨ä¼ æ„Ÿå™¨: {len(useful_sensors)} ä¸ª")
        
        # æ•°æ®æ ‡å‡†åŒ–
        self.train_df[self.feature_cols] = self.scaler.fit_transform(self.train_df[self.feature_cols])
        self.test_df[self.feature_cols] = self.scaler.transform(self.test_df[self.feature_cols])
        
        print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
    
    def create_statistical_features(self):
        """åˆ›å»ºç»Ÿè®¡ç‰¹å¾ - å¤ç”¨enhanced_hybrid_rul_systemçš„é€»è¾‘"""
        print(f"ğŸ”§ åˆ›å»ºç»Ÿè®¡ç‰¹å¾...")
        
        # 1. åˆ›å»ºåºåˆ—ç‰¹å¾ï¼ˆç”¨äºæå–ç»Ÿè®¡ç‰¹å¾ï¼‰
        X_train_seq, self.y_train = self._create_sequences(self.train_df, True)
        X_test_seq, _ = self._create_sequences(self.test_df, False)
        self.y_test = self.rul_df['RUL'].clip(upper=self.max_rul).values
        
        # 2. ä»åºåˆ—ç‰¹å¾æå–ç»Ÿè®¡ç‰¹å¾
        self.X_train_stat = np.array([self._extract_statistical_features(w) for w in X_train_seq])
        self.X_test_stat = np.array([self._extract_statistical_features(w) for w in X_test_seq])
        
        print(f"âœ… ç»Ÿè®¡ç‰¹å¾åˆ›å»ºå®Œæˆ:")
        print(f"   è®­ç»ƒç‰¹å¾: {self.X_train_stat.shape}")
        print(f"   æµ‹è¯•ç‰¹å¾: {self.X_test_stat.shape}")
        print(f"   è®­ç»ƒæ ‡ç­¾: {self.y_train.shape}")
        print(f"   æµ‹è¯•æ ‡ç­¾: {self.y_test.shape}")
        print(f"   ç‰¹å¾ç»´åº¦: {self.X_train_stat.shape[1]}")
    
    def _create_sequences(self, df, is_train=True):
        """åˆ›å»ºåºåˆ—æ•°æ®"""
        X, y = [], []
        
        for unit in df['unit_number'].unique():
            unit_data = df[df['unit_number'] == unit][self.feature_cols].values
            unit_rul = df[df['unit_number'] == unit]['RUL'].values if is_train else None
            
            # å¤„ç†çŸ­åºåˆ—
            if len(unit_data) < self.window_size:
                if len(unit_data) > 0:
                    padding_needed = self.window_size - len(unit_data)
                    padding = np.tile(unit_data[-1:], (padding_needed, 1))
                    unit_data = np.vstack([unit_data, padding])
                    if is_train:
                        unit_rul = np.concatenate([unit_rul, [unit_rul[-1]] * padding_needed])
                else:
                    continue
            
            if is_train:
                for i in range(len(unit_data) - self.window_size + 1):
                    X.append(unit_data[i:i+self.window_size])
                    y.append(unit_rul[i+self.window_size-1])
            else:
                X.append(unit_data[-self.window_size:])
        
        return np.array(X), np.array(y) if is_train else None
    
    def _extract_statistical_features(self, window):
        """æå–ç»Ÿè®¡ç‰¹å¾"""
        features = []
        
        for col_idx in range(window.shape[1]):
            col_data = window[:, col_idx]
            
            # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
            features.extend([
                np.mean(col_data),                    # å‡å€¼
                np.std(col_data),                     # æ ‡å‡†å·®
                np.max(col_data),                     # æœ€å¤§å€¼
                np.min(col_data),                     # æœ€å°å€¼
                np.median(col_data),                  # ä¸­ä½æ•°
                col_data[-1],                         # æœ€åå€¼
                col_data[0],                          # ç¬¬ä¸€å€¼
                col_data[-1] - col_data[0],          # æ€»å˜åŒ–é‡
                np.var(col_data),                     # æ–¹å·®
                np.percentile(col_data, 75) - np.percentile(col_data, 25),  # å››åˆ†ä½è·
            ])
            
            # è¶‹åŠ¿ç‰¹å¾
            if len(col_data) > 1:
                x = np.arange(len(col_data))
                slope = np.polyfit(x, col_data, 1)[0]
                features.append(slope)
                features.append(np.mean(np.abs(np.diff(col_data))))
            else:
                features.extend([0, 0])
        
        return features
    
    def calculate_nasa_score(self, y_true, y_pred):
        """è®¡ç®—NASAè¯„åˆ†"""
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
        
        a1 = 13  # æ—©é¢„æµ‹æƒ©ç½šå‚æ•°
        a2 = 10  # æ™šé¢„æµ‹æƒ©ç½šå‚æ•°
        
        scores = []
        for true_val, pred_val in zip(y_true, y_pred):
            d = pred_val - true_val
            if d < 0:  # æ—©é¢„æµ‹
                score = np.exp(-d / a1) - 1
            else:  # æ™šé¢„æµ‹
                score = np.exp(d / a2) - 1
            scores.append(score)
        
        return np.sum(scores)
    
    def calculate_phm_score(self, y_true, y_pred):
        """è®¡ç®—PHMè¯„åˆ†"""
        nasa_score = self.calculate_nasa_score(y_true, y_pred)
        return nasa_score / len(y_true)
    
    def train_xgboost_with_best_params(self):
        """ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒXGBoost"""
        print(f"\nğŸ¤– ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒXGBoost...")
        
        # æœ€ä½³å‚æ•°é…ç½®ï¼ˆåŸºäºæ‚¨æä¾›çš„å‚æ•°ï¼‰
        best_params = {
            'objective': 'reg:squarederror',
            'n_estimators': 500,
            'learning_rate': 0.02,
            'max_depth': 3,
            'subsample': 0.8,
            'colsample_bytree': 0.95,
            'reg_alpha': 0.1,
            'reg_lambda': 4,
            'random_state': 42,
            'n_jobs': -1
        }
        
        print(f"   ğŸ† ä½¿ç”¨å‚æ•°:")
        for key, value in best_params.items():
            print(f"      {key}: {value}")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = xgb.XGBRegressor(**best_params)
        
        # è®­ç»ƒè®¡æ—¶
        print(f"   ğŸš€ å¼€å§‹è®­ç»ƒ...")
        start_time = time.time()
        
        self.model.fit(self.X_train_stat, self.y_train)
        
        self.training_time = time.time() - start_time
        
        print(f"âœ… XGBoostè®­ç»ƒå®Œæˆ!")
        print(f"   â±ï¸ è®­ç»ƒæ—¶é—´: {self.training_time:.3f}ç§’")
        print(f"   ğŸŒ³ æ ‘çš„æ•°é‡: {self.model.n_estimators}")
        print(f"   ğŸ“Š è¾“å…¥ç‰¹å¾æ•°: {self.X_train_stat.shape[1]}")
        print(f"   ğŸ¯ æœ€å¤§æ·±åº¦: {self.model.max_depth}")
    
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print(f"\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # é¢„æµ‹è®¡æ—¶
        start_time = time.time()
        y_pred = self.model.predict(self.X_test_stat)
        self.prediction_time = time.time() - start_time
        
        # è®¡ç®—æŒ‡æ ‡
        rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))
        mae = mean_absolute_error(self.y_test, y_pred)
        r2 = r2_score(self.y_test, y_pred)
        nasa_score = self.calculate_nasa_score(self.y_test, y_pred)
        phm_score = self.calculate_phm_score(self.y_test, y_pred)
        
        # å­˜å‚¨ç»“æœ
        self.results = {
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'nasa_score': nasa_score,
            'phm_score': phm_score,
            'training_time': self.training_time,
            'prediction_time': self.prediction_time,
            'predictions': y_pred,
            'n_features': self.X_train_stat.shape[1],
            'n_train_samples': len(self.y_train),
            'n_test_samples': len(self.y_test)
        }
        
        print(f"âœ… æ¨¡å‹è¯„ä¼°å®Œæˆ!")
        print(f"   ğŸ“Š RMSE: {rmse:.3f}")
        print(f"   ğŸ“Š MAE: {mae:.3f}")
        print(f"   ğŸ“Š RÂ²: {r2:.4f}")
        print(f"   ğŸ¯ NASA Score: {nasa_score:.3f}")
        print(f"   ğŸ¯ PHM Score: {phm_score:.4f}")
        print(f"   â±ï¸ é¢„æµ‹æ—¶é—´: {self.prediction_time:.3f}ç§’")
        print(f"   âš¡ é¢„æµ‹é€Ÿåº¦: {len(self.y_test)/self.prediction_time:.1f} æ ·æœ¬/ç§’")
    
    def plot_results(self):
        """ç»˜åˆ¶ç»“æœå›¾è¡¨"""
        print(f"ğŸ“Š ç”Ÿæˆç»“æœå›¾è¡¨...")
        
        y_pred = self.results['predictions']
        
        # åˆ›å»ºå›¾å½¢
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'XGBoostå•ç‹¬æµ‹è¯•ç»“æœ - {self.dataset_name}', fontsize=16, fontweight='bold')
        
        # 1. é¢„æµ‹vsçœŸå®å€¼æ•£ç‚¹å›¾
        ax1 = axes[0, 0]
        ax1.scatter(self.y_test, y_pred, alpha=0.6, s=50, color='blue', edgecolors='navy', linewidth=0.5)
        ax1.plot([0, self.max_rul], [0, self.max_rul], 'r--', lw=2, label='Perfect Prediction')
        ax1.set_xlabel('çœŸå®RUL', fontsize=12, fontweight='bold')
        ax1.set_ylabel('é¢„æµ‹RUL', fontsize=12, fontweight='bold')
        ax1.set_title('é¢„æµ‹å€¼ vs çœŸå®å€¼', fontsize=14, fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æ·»åŠ æ€§èƒ½æŒ‡æ ‡æ–‡æœ¬
        metrics_text = f'RMSE: {self.results["rmse"]:.3f}\nMAE: {self.results["mae"]:.3f}\n' + \
                      f'RÂ²: {self.results["r2"]:.4f}\nNASA: {self.results["nasa_score"]:.2f}\n' + \
                      f'PHM: {self.results["phm_score"]:.4f}'
        ax1.text(0.05, 0.95, metrics_text, transform=ax1.transAxes, 
                verticalalignment='top', fontsize=10, fontweight='bold',
                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        # 2. æ—¶é—´åºåˆ—é¢„æµ‹å¯¹æ¯”å›¾
        ax2 = axes[0, 1]
        sample_indices = range(len(self.y_test))
        ax2.plot(sample_indices, self.y_test, 'b-', label='True RUL', linewidth=2, alpha=0.8)
        ax2.plot(sample_indices, y_pred, 'r-', label='Predicted RUL', linewidth=2, alpha=0.8)
        ax2.set_xlabel('Sample Index', fontsize=12, fontweight='bold')
        ax2.set_ylabel('RUL', fontsize=12, fontweight='bold')
        ax2.set_title('RULé¢„æµ‹æ—¶é—´åºåˆ—å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. é¢„æµ‹è¯¯å·®åˆ†å¸ƒ
        ax3 = axes[1, 0]
        errors = y_pred - self.y_test
        ax3.hist(errors, bins=30, alpha=0.7, color='green', edgecolor='darkgreen')
        ax3.axvline(np.mean(errors), color='red', linestyle='--', linewidth=2, 
                   label=f'Mean Error: {np.mean(errors):.3f}')
        ax3.set_xlabel('é¢„æµ‹è¯¯å·® (é¢„æµ‹å€¼ - çœŸå®å€¼)', fontsize=12, fontweight='bold')
        ax3.set_ylabel('é¢‘æ¬¡', fontsize=12, fontweight='bold')
        ax3.set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾
        ax4 = axes[1, 1]
        
        # æ€§èƒ½æŒ‡æ ‡ï¼ˆæ ‡å‡†åŒ–åˆ°0-1ï¼‰
        metrics_names = ['RÂ²', 'RMSE\n(åå‘)', 'MAE\n(åå‘)', 'PHM\n(åå‘)']
        
        # æ ‡å‡†åŒ–æŒ‡æ ‡å€¼ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
        r2_norm = max(0, self.results['r2'])  # RÂ²æœ¬èº«å°±æ˜¯0-1
        rmse_norm = max(0, 1 - self.results['rmse'] / 50)  # å‡è®¾RMSE=50ä¸ºæœ€å·®
        mae_norm = max(0, 1 - self.results['mae'] / 40)   # å‡è®¾MAE=40ä¸ºæœ€å·®
        phm_norm = max(0, 1 - self.results['phm_score'] / 10)  # å‡è®¾PHM=10ä¸ºæœ€å·®
        
        values = [r2_norm, rmse_norm, mae_norm, phm_norm]
        
        # åˆ›å»ºé›·è¾¾å›¾
        angles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False).tolist()
        values += values[:1]  # é—­åˆå›¾å½¢
        angles += angles[:1]
        
        ax4.plot(angles, values, 'o-', linewidth=2, color='blue', alpha=0.8)
        ax4.fill(angles, values, alpha=0.25, color='blue')
        ax4.set_xticks(angles[:-1])
        ax4.set_xticklabels(metrics_names)
        ax4.set_ylim(0, 1)
        ax4.set_title('æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾', fontsize=14, fontweight='bold')
        ax4.grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        plot_file = self.output_dir / f'xgboost_test_results_{self.dataset_name}.png'
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"   âœ… ç»“æœå›¾è¡¨å·²ä¿å­˜: {plot_file}")
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        print(f"ğŸ’¾ ä¿å­˜æµ‹è¯•ç»“æœ...")
        
        # ä¿å­˜æ¨¡å‹
        model_file = self.output_dir / f'xgboost_best_model_{self.dataset_name}.pkl'
        with open(model_file, 'wb') as f:
            pickle.dump(self.model, f)
        
        # ä¿å­˜ç»“æœJSON
        results_to_save = self.results.copy()
        results_to_save['predictions'] = results_to_save['predictions'].tolist()
        
        results_file = self.output_dir / f'xgboost_test_results_{self.dataset_name}.json'
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_to_save, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_file = self.output_dir / f'xgboost_test_report_{self.dataset_name}.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(f"XGBoostå•ç‹¬æµ‹è¯•æŠ¥å‘Š - {self.dataset_name}\n")
            f.write("="*50 + "\n\n")
            f.write(f"æ•°æ®é›†ä¿¡æ¯:\n")
            f.write(f"  è®­ç»ƒæ ·æœ¬æ•°: {self.results['n_train_samples']}\n")
            f.write(f"  æµ‹è¯•æ ·æœ¬æ•°: {self.results['n_test_samples']}\n")
            f.write(f"  ç‰¹å¾ç»´åº¦: {self.results['n_features']}\n\n")
            f.write(f"æ¨¡å‹å‚æ•°:\n")
            f.write(f"  æ ‘çš„æ•°é‡: {self.model.n_estimators}\n")
            f.write(f"  æœ€å¤§æ·±åº¦: {self.model.max_depth}\n")
            f.write(f"  å­¦ä¹ ç‡: {self.model.learning_rate}\n")
            f.write(f"  å­é‡‡æ ·ç‡: {self.model.subsample}\n")
            f.write(f"  ç‰¹å¾é‡‡æ ·ç‡: {self.model.colsample_bytree}\n\n")
            f.write(f"æ€§èƒ½æŒ‡æ ‡:\n")
            f.write(f"  RMSE: {self.results['rmse']:.4f}\n")
            f.write(f"  MAE: {self.results['mae']:.4f}\n")
            f.write(f"  RÂ²: {self.results['r2']:.4f}\n")
            f.write(f"  NASA Score: {self.results['nasa_score']:.4f}\n")
            f.write(f"  PHM Score: {self.results['phm_score']:.4f}\n\n")
            f.write(f"æ—¶é—´æ€§èƒ½:\n")
            f.write(f"  è®­ç»ƒæ—¶é—´: {self.results['training_time']:.3f}ç§’\n")
            f.write(f"  é¢„æµ‹æ—¶é—´: {self.results['prediction_time']:.3f}ç§’\n")
            f.write(f"  é¢„æµ‹é€Ÿåº¦: {len(self.y_test)/self.results['prediction_time']:.1f} æ ·æœ¬/ç§’\n")
        
        print(f"   âœ… æ¨¡å‹å·²ä¿å­˜: {model_file}")
        print(f"   âœ… ç»“æœå·²ä¿å­˜: {results_file}")
        print(f"   âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    def run_complete_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print(f"ğŸš€ å¼€å§‹XGBoostå®Œæ•´æµ‹è¯•")
        print("="*60)
        
        # 1. æ•°æ®å¤„ç†
        self.load_and_preprocess_data()
        self.create_statistical_features()
        
        # 2. è®­ç»ƒæ¨¡å‹
        self.train_xgboost_with_best_params()
        
        # 3. è¯„ä¼°æ€§èƒ½
        self.evaluate_model()
        
        # 4. ç”Ÿæˆå›¾è¡¨
        self.plot_results()
        
        # 5. ä¿å­˜ç»“æœ
        self.save_results()
        
        print(f"\nğŸ‰ XGBoostæµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
        print(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½æ‘˜è¦:")
        print(f"   RÂ² Score: {self.results['r2']:.4f}")
        print(f"   RMSE: {self.results['rmse']:.3f}")
        print(f"   è®­ç»ƒæ—¶é—´: {self.results['training_time']:.3f}ç§’")
        print(f"   é¢„æµ‹é€Ÿåº¦: {len(self.y_test)/self.results['prediction_time']:.1f} æ ·æœ¬/ç§’")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ XGBoostå•ç‹¬æ€§èƒ½æµ‹è¯•")
    print("="*40)
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = XGBoostSingleTester(dataset_name='FD001')
    
    # è¿è¡Œå®Œæ•´æµ‹è¯•
    tester.run_complete_test()

if __name__ == "__main__":
    main()