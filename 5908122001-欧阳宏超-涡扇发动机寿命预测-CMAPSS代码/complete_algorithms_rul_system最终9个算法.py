"""
å®Œæ•´çš„ä¸ƒç®—æ³•RULé¢„æµ‹ç³»ç»Ÿ
åŒ…å«ï¼šSVR, GRU, BiLSTM, Transformer, Full TFT, XGBoost, LightGBM
ç‰¹ç‚¹ï¼š
- å®Œæ•´çš„æ•°æ®å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
- å‚æ•°ä¼˜åŒ–å’Œé…ç½®ç®¡ç†
- è¯¦ç»†çš„å¯è§†åŒ–å’Œåˆ†æ
- æ¸…æ™°çš„ç®—æ³•é€»è¾‘å’Œæ¨¡å—åŒ–è®¾è®¡
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import xgboost as xgb
import lightgbm as lgb
import warnings
import time
import json
import pickle
from pathlib import Path
warnings.filterwarnings('ignore')

# PyTorch imports for deep learning models
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    import torch.nn.functional as F
    PYTORCH_AVAILABLE = True
    print(f"âœ… PyTorch {torch.__version__} å¯ç”¨")
    if torch.cuda.is_available():
        print(f"ğŸš€ CUDAå¯ç”¨: {torch.cuda.get_device_name(0)}")
except ImportError:
    PYTORCH_AVAILABLE = False
    print("âŒ PyTorchæœªå®‰è£…ï¼Œå°†è·³è¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹")

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
if PYTORCH_AVAILABLE:
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class CompleteSevenAlgorithmsRULSystem:
    """å®Œæ•´çš„ä¸ƒç®—æ³•RULé¢„æµ‹ç³»ç»Ÿ"""
    
    def __init__(self, dataset='FD001', window_size=30, max_rul=125, device='cuda'):
        self.dataset = dataset
        self.window_size = window_size
        self.max_rul = max_rul
        self.device = torch.device(device if torch.cuda.is_available() and PYTORCH_AVAILABLE else 'cpu')
        
        # æ•°æ®å­˜å‚¨
        self.train_df = None
        self.test_df = None
        self.rul_df = None
        self.feature_cols = None
        
        # ä¸åŒç±»å‹çš„æ•°æ®
        self.X_train_stat = None  # ç»Ÿè®¡ç‰¹å¾ - ç”¨äºä¼ ç»ŸML
        self.X_test_stat = None
        self.X_train_seq = None   # åºåˆ—æ•°æ® - ç”¨äºæ·±åº¦å­¦ä¹ 
        self.X_test_seq = None
        self.y_train = None
        self.y_test = None
        
        # æ ‡å‡†åŒ–å™¨
        self.scaler_stat = StandardScaler()
        self.scaler_seq = StandardScaler()
        
        # ç»“æœå­˜å‚¨
        self.models = {}
        self.results = {}
        self.best_params = {}
        self.training_history = {}
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(f'results_{dataset}')
        self.output_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ¯ å®Œæ•´ä¸ƒç®—æ³•RULé¢„æµ‹ç³»ç»Ÿåˆå§‹åŒ–")
        print(f"ğŸ“Š æ•°æ®é›†: {dataset}")
        print(f"ğŸ¤– ç®—æ³•: SVR, GRU, BiLSTM, Transformer, Full TFT, XGBoost, LightGBM")
        print(f"ğŸ’» è®¾å¤‡: {self.device}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
    
    # ========== æ•°æ®å¤„ç†æ¨¡å— ==========
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print(f"ğŸ“Š åŠ è½½ {self.dataset} æ•°æ®é›†...")
        
        cols = ['unit_number', 'time_cycles', 'op_setting_1', 'op_setting_2', 'op_setting_3'] + \
               [f'sensor_{i}' for i in range(1, 22)]
        
        self.train_df = pd.read_csv(f'train_{self.dataset}.txt', sep=r'\s+', header=None, names=cols)
        self.test_df = pd.read_csv(f'test_{self.dataset}.txt', sep=r'\s+', header=None, names=cols)
        self.rul_df = pd.read_csv(f'RUL_{self.dataset}.txt', sep=r'\s+', header=None, names=['RUL'])
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
        print(f"   è®­ç»ƒé›†: {self.train_df.shape}")
        print(f"   æµ‹è¯•é›†: {self.test_df.shape}")
        print(f"   RULæ ‡ç­¾: {self.rul_df.shape}")
    
    def preprocess_data(self):
        """æ•°æ®é¢„å¤„ç†"""
        print(f"ğŸ”„ æ•°æ®é¢„å¤„ç†...")
        
        # è®¡ç®—RUL
        def calculate_rul(group):
            group = group.copy()
            group['RUL'] = group['time_cycles'].max() - group['time_cycles']
            group['RUL'] = group['RUL'].clip(upper=self.max_rul)
            return group
        
        self.train_df = self.train_df.groupby('unit_number').apply(calculate_rul).reset_index(drop=True)
        
        # é€‰æ‹©æœ‰ç”¨çš„ä¼ æ„Ÿå™¨
        sensor_cols = [col for col in self.train_df.columns if 'sensor' in col]
        std_values = self.train_df[sensor_cols].std()
        useful_sensors = std_values[std_values > 0].index.tolist()
        
        self.feature_cols = ['op_setting_1', 'op_setting_2', 'op_setting_3'] + useful_sensors
        
        print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ")
        print(f"   ç‰¹å¾æ•°: {len(self.feature_cols)}")
        print(f"   æœ‰ç”¨ä¼ æ„Ÿå™¨: {len(useful_sensors)}")
    
    def extract_comprehensive_features(self, window):
        """æå–comprehensiveç‰ˆæœ¬çš„å®Œæ•´ç»Ÿè®¡ç‰¹å¾ - 12ä¸ªç‰¹å¾/åˆ—"""
        features = []
        for col in range(window.shape[1]):
            col_data = window[:, col]
            if len(col_data) == 0:
                features.extend([0] * 12)
                continue
                
            features.extend([
                np.mean(col_data),                    # å‡å€¼
                np.std(col_data),                     # æ ‡å‡†å·®
                np.max(col_data),                     # æœ€å¤§å€¼
                np.min(col_data),                     # æœ€å°å€¼
                np.ptp(col_data),                     # æå·®
                np.median(col_data),                  # ä¸­ä½æ•°
                np.percentile(col_data, 25),          # 25åˆ†ä½æ•°
                np.percentile(col_data, 75),          # 75åˆ†ä½æ•°
                np.sum(np.diff(col_data) > 0) / max(len(col_data) - 1, 1) if len(col_data) > 1 else 0,  # ä¸Šå‡è¶‹åŠ¿æ¯”ä¾‹
                np.var(col_data),                     # æ–¹å·®
                np.sum(np.abs(np.diff(col_data))) / max(len(col_data) - 1, 1) if len(col_data) > 1 else 0,  # å¹³å‡ç»å¯¹å˜åŒ–
                col_data[-1] - col_data[0] if len(col_data) > 1 else 0,  # æ€»å˜åŒ–é‡
            ])
        return features
    
    def create_statistical_data(self, df, is_train=True):
        """åˆ›å»ºç»Ÿè®¡ç‰¹å¾æ•°æ® - ç”¨äºä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•"""
        X = []
        y = []
        
        for unit in df['unit_number'].unique():
            unit_data = df[df['unit_number'] == unit][self.feature_cols].values
            unit_rul = df[df['unit_number'] == unit]['RUL'].values if is_train else None
            
            # å¤„ç†æ•°æ®é•¿åº¦ä¸è¶³çš„æƒ…å†µ
            if len(unit_data) < self.window_size:
                if len(unit_data) > 0:
                    padding_needed = self.window_size - len(unit_data)
                    last_row = unit_data[-1:]
                    padding = np.tile(last_row, (padding_needed, 1))
                    unit_data = np.vstack([unit_data, padding])
                    
                    if is_train and unit_rul is not None:
                        last_rul = unit_rul[-1]
                        unit_rul = np.concatenate([unit_rul, [last_rul] * padding_needed])
                else:
                    unit_data = np.zeros((self.window_size, len(self.feature_cols)))
                    if is_train:
                        unit_rul = np.zeros(self.window_size)
            
            if is_train:
                # è®­ç»ƒé›†ï¼šåˆ›å»ºæ»‘åŠ¨çª—å£
                for i in range(len(unit_data) - self.window_size + 1):
                    window = unit_data[i:i+self.window_size]
                    X.append(self.extract_comprehensive_features(window))
                    if unit_rul is not None and len(unit_rul) > i+self.window_size-1:
                        y.append(unit_rul[i+self.window_size-1])
            else:
                # æµ‹è¯•é›†ï¼šåªå–æœ€åä¸€ä¸ªçª—å£
                window = unit_data[-self.window_size:]
                X.append(self.extract_comprehensive_features(window))
        
        return np.array(X), np.array(y) if is_train else None
    
    def create_sequence_data(self, df, is_train=True):
        """åˆ›å»ºçœŸæ­£çš„åºåˆ—æ•°æ® - ç”¨äºæ·±åº¦å­¦ä¹ ç®—æ³•"""
        X = []
        y = []
        
        for unit in df['unit_number'].unique():
            unit_data = df[df['unit_number'] == unit][self.feature_cols].values
            unit_rul = df[df['unit_number'] == unit]['RUL'].values if is_train else None
            
            # å¤„ç†æ•°æ®é•¿åº¦ä¸è¶³çš„æƒ…å†µ
            if len(unit_data) < self.window_size:
                if len(unit_data) > 0:
                    padding_needed = self.window_size - len(unit_data)
                    last_row = unit_data[-1:]
                    padding = np.tile(last_row, (padding_needed, 1))
                    unit_data = np.vstack([unit_data, padding])
                    
                    if is_train and unit_rul is not None:
                        last_rul = unit_rul[-1]
                        unit_rul = np.concatenate([unit_rul, [last_rul] * padding_needed])
                else:
                    unit_data = np.zeros((self.window_size, len(self.feature_cols)))
                    if is_train:
                        unit_rul = np.zeros(self.window_size)
            
            if is_train:
                # è®­ç»ƒé›†ï¼šåˆ›å»ºæ»‘åŠ¨çª—å£ï¼Œä¿æŒåºåˆ—ç»“æ„
                for i in range(len(unit_data) - self.window_size + 1):
                    window = unit_data[i:i+self.window_size]  # ä¿æŒ (window_size, n_features) å½¢çŠ¶
                    X.append(window)
                    if unit_rul is not None and len(unit_rul) > i+self.window_size-1:
                        y.append(unit_rul[i+self.window_size-1])
            else:
                # æµ‹è¯•é›†ï¼šåªå–æœ€åä¸€ä¸ªçª—å£
                window = unit_data[-self.window_size:]  # (window_size, n_features)
                X.append(window)
        
        return np.array(X), np.array(y) if is_train else None
    
    def feature_engineering(self):
        """ç‰¹å¾å·¥ç¨‹ - ä¸ºä¸åŒç®—æ³•å‡†å¤‡æ­£ç¡®çš„æ•°æ®æ ¼å¼"""
        print(f"ğŸ”§ ç‰¹å¾å·¥ç¨‹...")
        
        # 1. åˆ›å»ºç»Ÿè®¡ç‰¹å¾æ•°æ® (ç”¨äºSVR, XGBoost, LightGBM)
        print(f"ğŸ“Š ä¸ºä¼ ç»ŸMLç®—æ³•åˆ›å»ºç»Ÿè®¡ç‰¹å¾æ•°æ®...")
        self.X_train_stat, self.y_train = self.create_statistical_data(self.train_df, is_train=True)
        self.X_test_stat, _ = self.create_statistical_data(self.test_df, is_train=False)
        self.y_test = self.rul_df['RUL'].clip(upper=self.max_rul).values
        
        # 2. åˆ›å»ºåºåˆ—æ•°æ® (ç”¨äºGRU, BiLSTM, Transformer, TFT)
        print(f"ğŸ“Š ä¸ºæ·±åº¦å­¦ä¹ ç®—æ³•åˆ›å»ºåºåˆ—æ•°æ®...")
        X_train_seq_raw, y_train_seq = self.create_sequence_data(self.train_df, is_train=True)
        X_test_seq_raw, _ = self.create_sequence_data(self.test_df, is_train=False)
        
        # 3. æ ‡å‡†åŒ–ç»Ÿè®¡ç‰¹å¾
        self.X_train_stat = self.scaler_stat.fit_transform(self.X_train_stat)
        self.X_test_stat = self.scaler_stat.transform(self.X_test_stat)
        
        # 4. æ ‡å‡†åŒ–åºåˆ—æ•°æ®
        n_samples, n_timesteps, n_features = X_train_seq_raw.shape
        X_train_flat = X_train_seq_raw.reshape(-1, n_features)
        X_train_scaled = self.scaler_seq.fit_transform(X_train_flat)
        self.X_train_seq = X_train_scaled.reshape(n_samples, n_timesteps, n_features)
        
        n_samples_test = X_test_seq_raw.shape[0]
        X_test_flat = X_test_seq_raw.reshape(-1, n_features)
        X_test_scaled = self.scaler_seq.transform(X_test_flat)
        self.X_test_seq = X_test_scaled.reshape(n_samples_test, n_timesteps, n_features)
        
        print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ:")
        print(f"   ç»Ÿè®¡ç‰¹å¾ (ä¼ ç»ŸML): è®­ç»ƒ{self.X_train_stat.shape}, æµ‹è¯•{self.X_test_stat.shape}")
        print(f"   åºåˆ—æ•°æ® (æ·±åº¦å­¦ä¹ ): è®­ç»ƒ{self.X_train_seq.shape}, æµ‹è¯•{self.X_test_seq.shape}")
        print(f"   ç›®æ ‡å˜é‡: è®­ç»ƒ{self.y_train.shape}, æµ‹è¯•{self.y_test.shape}")
    
    # ========== è¯„åˆ†è®¡ç®—æ¨¡å— ==========
    def calculate_nasa_score(self, y_true, y_pred):
        """è®¡ç®—NASAè¯„åˆ†"""
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
        
        a1 = 13  # æ—©é¢„æµ‹æƒ©ç½šå‚æ•°
        a2 = 10  # æ™šé¢„æµ‹æƒ©ç½šå‚æ•°
        
        d_i = np.abs(y_true - y_pred)
        early_mask = y_pred < y_true
        late_mask = y_pred >= y_true
        
        scores = np.zeros_like(d_i, dtype=float)
        
        if np.any(early_mask):
            scores[early_mask] = np.exp(-d_i[early_mask] / a1) - 1
        
        if np.any(late_mask):
            scores[late_mask] = np.exp(d_i[late_mask] / a2) - 1
        
        return np.sum(scores)
    
    def calculate_phm_score(self, y_true, y_pred):
        """è®¡ç®—PHMè¯„åˆ†"""
        nasa_score = self.calculate_nasa_score(y_true, y_pred)
        return nasa_score / len(y_true)
    
    # ========== ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•æ¨¡å— ==========
    def train_svr(self):
        """è®­ç»ƒSVRç®—æ³•"""
        print(f"ğŸ¤– è®­ç»ƒSVRç®—æ³•...")
        start_time = time.time()
        
        # å‚æ•°ç½‘æ ¼
        param_grid = {
            'kernel': ['rbf'],
            'C': [0.1, 1, 10, 100, 1000],
            'gamma': [1e-4, 1e-3, 1e-2, 1e-1, 1, 'scale', 'auto'],
            'epsilon': [0.01, 0.1, 0.2, 0.5, 1.0]
        }
        
        print(f"   ğŸ” æ‰§è¡Œç½‘æ ¼æœç´¢...")
        svr = SVR()
        grid_search = GridSearchCV(
            svr, param_grid, cv=3, 
            scoring='neg_mean_squared_error', 
            n_jobs=-1
        )
        
        grid_search.fit(self.X_train_stat, self.y_train)
        
        # è·å–æœ€ä½³æ¨¡å‹
        best_svr = grid_search.best_estimator_
        best_params = grid_search.best_params_
        
        # é¢„æµ‹
        y_pred = best_svr.predict(self.X_test_stat)
        
        # è®¡ç®—æŒ‡æ ‡
        rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))
        mae = mean_absolute_error(self.y_test, y_pred)
        r2 = r2_score(self.y_test, y_pred)
        nasa_score = self.calculate_nasa_score(self.y_test, y_pred)
        phm_score = self.calculate_phm_score(self.y_test, y_pred)
        
        training_time = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        self.models['SVR'] = best_svr
        self.best_params['SVR'] = best_params
        self.results['SVR'] = {
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'nasa_score': nasa_score,
            'phm_score': phm_score,
            'training_time': training_time,
            'predictions': y_pred,
            'best_params': best_params
        }
        
        print(f"âœ… SVRè®­ç»ƒå®Œæˆ!")
        print(f"   ğŸ† æœ€ä½³å‚æ•°: {best_params}")
        print(f"   ğŸ“Š RMSE: {rmse:.3f}, MAE: {mae:.3f}, RÂ²: {r2:.4f}")
        print(f"   ğŸ¯ NASA Score: {nasa_score:.3f}, PHM Score: {phm_score:.4f}")
        print(f"   â±ï¸ è®­ç»ƒæ—¶é—´: {training_time:.1f}ç§’")
    
    def train_xgboost(self):
        """è®­ç»ƒXGBoostç®—æ³•"""
        print(f"ğŸ¤– è®­ç»ƒXGBoostç®—æ³•...")
        start_time = time.time()
        
        # å‚æ•°ç½‘æ ¼
        param_grid = {
            'n_estimators': [100, 200, 300],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 6, 9],
            'subsample': [0.8, 0.9, 1.0],
            'colsample_bytree': [0.8, 0.9, 1.0]
        }
        
        print(f"   ğŸ” æ‰§è¡Œéšæœºæœç´¢...")
        xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1)
        random_search = RandomizedSearchCV(
            xgb_model, param_grid, n_iter=20, cv=3,
            scoring='neg_mean_squared_error', 
            n_jobs=-1, random_state=42
        )
        
        random_search.fit(self.X_train_stat, self.y_train)
        
        # è·å–æœ€ä½³æ¨¡å‹
        best_xgb = random_search.best_estimator_
        best_params = random_search.best_params_
        
        # é¢„æµ‹
        y_pred = best_xgb.predict(self.X_test_stat)
        
        # è®¡ç®—æŒ‡æ ‡
        rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))
        mae = mean_absolute_error(self.y_test, y_pred)
        r2 = r2_score(self.y_test, y_pred)
        nasa_score = self.calculate_nasa_score(self.y_test, y_pred)
        phm_score = self.calculate_phm_score(self.y_test, y_pred)
        
        training_time = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        self.models['XGBoost'] = best_xgb
        self.best_params['XGBoost'] = best_params
        self.results['XGBoost'] = {
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'nasa_score': nasa_score,
            'phm_score': phm_score,
            'training_time': training_time,
            'predictions': y_pred,
            'best_params': best_params
        }
        
        print(f"âœ… XGBoostè®­ç»ƒå®Œæˆ!")
        print(f"   ğŸ† æœ€ä½³å‚æ•°: {best_params}")
        print(f"   ğŸ“Š RMSE: {rmse:.3f}, MAE: {mae:.3f}, RÂ²: {r2:.4f}")
        print(f"   ğŸ¯ NASA Score: {nasa_score:.3f}, PHM Score: {phm_score:.4f}")
        print(f"   â±ï¸ è®­ç»ƒæ—¶é—´: {training_time:.1f}ç§’")
    
    def train_lightgbm(self):
        """è®­ç»ƒLightGBMç®—æ³•"""
        print(f"ğŸ¤– è®­ç»ƒLightGBMç®—æ³•...")
        start_time = time.time()
        
        # å‚æ•°ç½‘æ ¼
        param_grid = {
            'n_estimators': [100, 200, 300],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 6, 9],
            'num_leaves': [31, 50, 100],
            'subsample': [0.8, 0.9, 1.0]
        }
        
        print(f"   ğŸ” æ‰§è¡Œéšæœºæœç´¢...")
        lgb_model = lgb.LGBMRegressor(random_state=42, verbose=-1, n_jobs=-1)
        random_search = RandomizedSearchCV(
            lgb_model, param_grid, n_iter=20, cv=3,
            scoring='neg_mean_squared_error', 
            n_jobs=-1, random_state=42
        )
        
        random_search.fit(self.X_train_stat, self.y_train)
        
        # è·å–æœ€ä½³æ¨¡å‹
        best_lgb = random_search.best_estimator_
        best_params = random_search.best_params_
        
        # é¢„æµ‹
        y_pred = best_lgb.predict(self.X_test_stat)
        
        # è®¡ç®—æŒ‡æ ‡
        rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))
        mae = mean_absolute_error(self.y_test, y_pred)
        r2 = r2_score(self.y_test, y_pred)
        nasa_score = self.calculate_nasa_score(self.y_test, y_pred)
        phm_score = self.calculate_phm_score(self.y_test, y_pred)
        
        training_time = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        self.models['LightGBM'] = best_lgb
        self.best_params['LightGBM'] = best_params
        self.results['LightGBM'] = {
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'nasa_score': nasa_score,
            'phm_score': phm_score,
            'training_time': training_time,
            'predictions': y_pred,
            'best_params': best_params
        }
        
        print(f"âœ… LightGBMè®­ç»ƒå®Œæˆ!")
        print(f"   ğŸ† æœ€ä½³å‚æ•°: {best_params}")
        print(f"   ğŸ“Š RMSE: {rmse:.3f}, MAE: {mae:.3f}, RÂ²: {r2:.4f}")
        print(f"   ğŸ¯ NASA Score: {nasa_score:.3f}, PHM Score: {phm_score:.4f}")
        print(f"   â±ï¸ è®­ç»ƒæ—¶é—´: {training_time:.1f}ç§’")
    
    # ========== æ·±åº¦å­¦ä¹ ç®—æ³•æ¨¡å— ==========
    
    # GRUæ¨¡å‹å®šä¹‰
    class OptimizedGRUModel(nn.Module):
        """ä¼˜åŒ–çš„GRUæ¨¡å‹"""
        def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3, 
                     use_batch_norm=True, use_residual=False):
            super().__init__()
            self.hidden_size = hidden_size
            self.num_layers = num_layers
            self.use_batch_norm = use_batch_norm
            self.use_residual = use_residual
            
            self.gru = nn.GRU(input_size, hidden_size, num_layers,
                             batch_first=True, dropout=dropout if num_layers > 1 else 0, 
                             bidirectional=False)
            
            self.dropout = nn.Dropout(dropout)
            
            if use_batch_norm:
                self.batch_norm = nn.BatchNorm1d(hidden_size)
            
            # å…¨è¿æ¥å±‚
            self.fc_layers = nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_size // 2, hidden_size // 4),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_size // 4, 1)
            )
            
            # æ®‹å·®è¿æ¥
            if use_residual:
                self.residual_fc = nn.Linear(input_size, 1)
            
        def forward(self, x):
            gru_out, _ = self.gru(x)
            last_output = gru_out[:, -1, :]
            
            if self.use_batch_norm and last_output.size(0) > 1:
                last_output = self.batch_norm(last_output)
            
            output = self.fc_layers(last_output)
            
            # æ®‹å·®è¿æ¥
            if self.use_residual:
                residual = self.residual_fc(x.mean(dim=1))
                output = output + residual
            
            return output
    
    # BiLSTMæ¨¡å‹å®šä¹‰
    class OptimizedBiLSTMModel(nn.Module):
        """ä¼˜åŒ–çš„åŒå‘LSTMæ¨¡å‹"""
        def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3,
                     use_batch_norm=True, use_attention=False):
            super().__init__()
            self.hidden_size = hidden_size
            self.num_layers = num_layers
            self.use_batch_norm = use_batch_norm
            self.use_attention = use_attention
            
            self.bilstm = nn.LSTM(input_size, hidden_size, num_layers,
                                 batch_first=True, dropout=dropout if num_layers > 1 else 0,
                                 bidirectional=True)
            
            self.dropout = nn.Dropout(dropout)
            
            # åŒå‘LSTMè¾“å‡ºç»´åº¦æ˜¯hidden_size * 2
            lstm_output_size = hidden_size * 2
            
            if use_batch_norm:
                self.batch_norm = nn.BatchNorm1d(lstm_output_size)
            
            # æ³¨æ„åŠ›æœºåˆ¶
            if use_attention:
                self.attention = nn.MultiheadAttention(lstm_output_size, num_heads=8, dropout=dropout)
                self.layer_norm = nn.LayerNorm(lstm_output_size)
            
            # å…¨è¿æ¥å±‚
            self.fc_layers = nn.Sequential(
                nn.Linear(lstm_output_size, lstm_output_size // 2),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(lstm_output_size // 2, lstm_output_size // 4),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(lstm_output_size // 4, 1)
            )
            
        def forward(self, x):
            lstm_out, _ = self.bilstm(x)
            
            if self.use_attention:
                # ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶
                lstm_out_transposed = lstm_out.transpose(0, 1)  # (seq_len, batch, features)
                attn_out, _ = self.attention(lstm_out_transposed, lstm_out_transposed, lstm_out_transposed)
                attn_out = attn_out.transpose(0, 1)  # (batch, seq_len, features)
                attn_out = self.layer_norm(attn_out + lstm_out)
                last_output = attn_out[:, -1, :]
            else:
                last_output = lstm_out[:, -1, :]
            
            if self.use_batch_norm and last_output.size(0) > 1:
                last_output = self.batch_norm(last_output)
            
            output = self.fc_layers(last_output)
            return output
    
    # Transformeræ¨¡å‹å®šä¹‰
    class OptimizedTransformerModel(nn.Module):
        """ä¼˜åŒ–çš„Transformeræ¨¡å‹"""
        def __init__(self, input_size, d_model=128, nhead=8, num_layers=4, dropout=0.3,
                     use_positional_encoding=True, use_layer_norm=True):
            super().__init__()
            self.input_size = input_size
            self.d_model = d_model
            self.use_positional_encoding = use_positional_encoding
            self.use_layer_norm = use_layer_norm
            
            # è¾“å…¥æŠ•å½±
            self.input_projection = nn.Linear(input_size, d_model)
            
            # ä½ç½®ç¼–ç 
            if use_positional_encoding:
                self.pos_encoding = nn.Parameter(torch.randn(1000, d_model) * 0.1)
            
            # Transformerç¼–ç å™¨
            encoder_layer = nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=nhead,
                dim_feedforward=d_model * 4,
                dropout=dropout,
                activation='gelu',
                batch_first=True
            )
            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
            
            # è¾“å‡ºå±‚
            self.dropout = nn.Dropout(dropout)
            
            if use_layer_norm:
                self.layer_norm = nn.LayerNorm(d_model)
            
            self.fc_layers = nn.Sequential(
                nn.Linear(d_model, d_model // 2),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(d_model // 2, d_model // 4),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(d_model // 4, 1)
            )
            
        def forward(self, x):
            batch_size, seq_len, _ = x.shape
            
            # è¾“å…¥æŠ•å½±
            x = self.input_projection(x)
            
            # æ·»åŠ ä½ç½®ç¼–ç 
            if self.use_positional_encoding:
                x = x + self.pos_encoding[:seq_len, :].unsqueeze(0)
            
            # Transformerç¼–ç 
            x = self.transformer(x)
            
            # å…¨å±€å¹³å‡æ± åŒ– + æœ€åæ—¶é—´æ­¥
            global_avg = x.mean(dim=1)
            last_step = x[:, -1, :]
            
            # ç»„åˆç‰¹å¾
            combined = global_avg + last_step
            
            if self.use_layer_norm:
                combined = self.layer_norm(combined)
            
            combined = self.dropout(combined)
            
            # è¾“å‡º
            output = self.fc_layers(combined)
            return output
    
    # Full TFTæ¨¡å‹å®šä¹‰
    class FullTemporalFusionTransformer(nn.Module):
        """å®Œæ•´ç‰ˆTemporal Fusion Transformer - å¢å¼ºç‰ˆå®ç°"""
        def __init__(self, input_size, hidden_size=128, num_heads=8, num_layers=3, dropout=0.3):
            super().__init__()
            
            # å¯¼å…¥å¢å¼ºç‰ˆTFT
            try:
                from enhanced_tft_model import EnhancedTemporalFusionTransformer
                
                # ä½¿ç”¨å¢å¼ºç‰ˆTFTä½œä¸ºæ ¸å¿ƒ
                self.enhanced_tft = EnhancedTemporalFusionTransformer(
                    seq_input_size=input_size,
                    static_input_size=None,  # åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸­ä¸ä½¿ç”¨é™æ€ç‰¹å¾
                    hidden_size=hidden_size,
                    num_heads=num_heads,
                    num_layers=num_layers,
                    dropout=dropout,
                    use_static_features=False
                )
                self.use_enhanced = True
                print("âœ… ä½¿ç”¨å¢å¼ºç‰ˆTFTå®ç°")
                
            except ImportError:
                print("âš ï¸ å¢å¼ºç‰ˆTFTä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€å®ç°")
                self.use_enhanced = False
                self._build_basic_tft(input_size, hidden_size, num_heads, num_layers, dropout)
        
        def _build_basic_tft(self, input_size, hidden_size, num_heads, num_layers, dropout):
            """æ„å»ºåŸºç¡€TFTå®ç°ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
            self.input_size = input_size
            self.hidden_size = hidden_size
            
            # Variable Selection Network
            self.variable_selection = nn.Sequential(
                nn.Linear(input_size, hidden_size),
                nn.ReLU(),
                nn.Linear(hidden_size, input_size),
                nn.Sigmoid()
            )
            
            # LSTM Encoder-Decoder
            self.lstm_encoder = nn.LSTM(input_size, hidden_size, batch_first=True)
            self.lstm_decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)
            
            # Multi-Head Attention
            self.multihead_attn = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout)
            
            # Gated Residual Network
            self.grn1 = self._build_grn(hidden_size)
            self.grn2 = self._build_grn(hidden_size)
            
            # Static Enrichment
            self.static_enrichment = nn.Sequential(
                nn.Linear(hidden_size, hidden_size),
                nn.ReLU(),
                nn.Dropout(dropout)
            )
            
            # Temporal Self-Attention
            self.temporal_attention = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout)
            
            # Position-wise Feed Forward
            self.feed_forward = nn.Sequential(
                nn.Linear(hidden_size, hidden_size * 4),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_size * 4, hidden_size)
            )
            
            # Output layers
            self.layer_norm = nn.LayerNorm(hidden_size)
            self.output_projection = nn.Sequential(
                nn.Linear(hidden_size, hidden_size // 2),
                nn.ReLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_size // 2, 1)
            )
            
        def _build_grn(self, input_size):
            """æ„å»ºGated Residual Network"""
            return nn.Sequential(
                nn.Linear(input_size, input_size),
                nn.ELU(),
                nn.Linear(input_size, input_size),
                nn.Dropout(0.1),
                nn.Linear(input_size, input_size * 2)  # ç”¨äºé—¨æ§æœºåˆ¶
            )
        
        def _apply_grn(self, x, grn):
            """åº”ç”¨Gated Residual Network"""
            grn_output = grn(x)
            gate, transform = torch.chunk(grn_output, 2, dim=-1)
            gate = torch.sigmoid(gate)
            return gate * transform + (1 - gate) * x
        
        def forward(self, x):
            if self.use_enhanced:
                # ä½¿ç”¨å¢å¼ºç‰ˆTFT
                outputs = self.enhanced_tft(x, None)
                return outputs['prediction']
            else:
                # ä½¿ç”¨åŸºç¡€å®ç°
                return self._forward_basic(x)
        
        def _forward_basic(self, x):
            """åŸºç¡€TFTå‰å‘ä¼ æ’­"""
            batch_size, seq_len, _ = x.shape
            
            # Variable Selection
            variable_weights = self.variable_selection(x)
            x_selected = x * variable_weights
            
            # LSTM Encoding
            lstm_out, (h_n, c_n) = self.lstm_encoder(x_selected)
            
            # Static Enrichment
            enriched = self.static_enrichment(lstm_out)
            
            # Temporal Self-Attention
            # è½¬æ¢ä¸º (seq_len, batch_size, hidden_size) for attention
            attn_input = enriched.transpose(0, 1)
            attn_output, _ = self.temporal_attention(attn_input, attn_input, attn_input)
            attn_output = attn_output.transpose(0, 1)  # è½¬å› (batch_size, seq_len, hidden_size)
            
            # Apply GRN
            grn_output = self._apply_grn(attn_output, self.grn1)
            
            # Position-wise Feed Forward
            ff_output = self.feed_forward(grn_output)
            
            # Apply second GRN
            grn_output2 = self._apply_grn(ff_output, self.grn2)
            
            # Layer normalization
            normalized = self.layer_norm(grn_output2)
            
            # Global average pooling
            pooled = torch.mean(normalized, dim=1)
            
            # Final prediction
            output = self.output_projection(pooled)
            
            return output
            static_context = self.static_enrichment(h_n[-1])  # ä½¿ç”¨æœ€åä¸€ä¸ªéšè—çŠ¶æ€
            
            # å°†é™æ€ä¸Šä¸‹æ–‡å¹¿æ’­åˆ°æ‰€æœ‰æ—¶é—´æ­¥
            static_context_expanded = static_context.unsqueeze(1).expand(-1, seq_len, -1)
            enriched_lstm = lstm_out + static_context_expanded
            
            # Gated Residual Network 1
            grn1_out = self._apply_grn(enriched_lstm, self.grn1)
            
            # Temporal Self-Attention
            grn1_transposed = grn1_out.transpose(0, 1)  # (seq_len, batch, hidden)
            attn_out, _ = self.temporal_attention(grn1_transposed, grn1_transposed, grn1_transposed)
            attn_out = attn_out.transpose(0, 1)  # (batch, seq_len, hidden)
            
            # Residual connection
            attn_out = self.layer_norm(attn_out + grn1_out)
            
            # Gated Residual Network 2
            grn2_out = self._apply_grn(attn_out, self.grn2)
            
            # Position-wise Feed Forward
            ff_out = self.feed_forward(grn2_out)
            
            # Final residual connection
            final_out = self.layer_norm(ff_out + grn2_out)
            
            # Global average pooling + last timestep
            global_avg = final_out.mean(dim=1)
            last_timestep = final_out[:, -1, :]
            combined = global_avg + last_timestep
            
            # Output projection
            output = self.output_projection(combined)
            return output
    
    def train_pytorch_model(self, model_class, model_name, **model_kwargs):
        """è®­ç»ƒPyTorchæ·±åº¦å­¦ä¹ æ¨¡å‹çš„é€šç”¨å‡½æ•°"""
        if not PYTORCH_AVAILABLE:
            print(f"âŒ PyTorchä¸å¯ç”¨ï¼Œè·³è¿‡{model_name}æ¨¡å‹")
            return
        
        print(f"ğŸ§  è®­ç»ƒ{model_name}æ¨¡å‹...")
        start_time = time.time()
        
        # ä½¿ç”¨æ­£ç¡®çš„åºåˆ—æ•°æ®
        X_train = torch.FloatTensor(self.X_train_seq).to(self.device)
        y_train = torch.FloatTensor(self.y_train).to(self.device)
        X_test = torch.FloatTensor(self.X_test_seq).to(self.device)
        
        # éªŒè¯é›†åˆ†å‰²
        val_size = int(0.2 * len(X_train))
        indices = torch.randperm(len(X_train))
        train_indices = indices[val_size:]
        val_indices = indices[:val_size]
        
        X_train_split = X_train[train_indices]
        y_train_split = y_train[train_indices]
        X_val = X_train[val_indices]
        y_val = y_train[val_indices]
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        batch_size = model_kwargs.get('batch_size', 32)
        train_dataset = TensorDataset(X_train_split, y_train_split)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        val_dataset = TensorDataset(X_val, y_val)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
        
        # åˆ›å»ºæ¨¡å‹
        input_size = self.X_train_seq.shape[2]
        model_params = {k: v for k, v in model_kwargs.items() if k not in ['lr', 'batch_size', 'epochs', 'weight_decay']}
        model = model_class(input_size=input_size, **model_params).to(self.device)
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        lr = model_kwargs.get('lr', 0.001)
        weight_decay = model_kwargs.get('weight_decay', 0.01)
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10, factor=0.5)
        criterion = nn.MSELoss()
        
        # æ—©åœå‚æ•°
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 15
        
        # è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        
        print(f"   ğŸ“Š è®­ç»ƒé›†: {len(train_indices)}, éªŒè¯é›†: {len(val_indices)}")
        print(f"   ğŸ—ï¸ æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
        
        # è®­ç»ƒå¾ªç¯
        epochs = model_kwargs.get('epochs', 100)
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0
            train_batches = 0
            
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X).squeeze()
                loss = criterion(outputs, batch_y)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                train_loss += loss.item()
                train_batches += 1
            
            avg_train_loss = train_loss / train_batches
            train_losses.append(avg_train_loss)
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0
            val_batches = 0
            
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = model(batch_X).squeeze()
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
                    val_batches += 1
            
            avg_val_loss = val_loss / val_batches
            val_losses.append(avg_val_loss)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(avg_val_loss)
            
            # æ—©åœæ£€æŸ¥
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(model.state_dict(), self.output_dir / f'best_{model_name.lower()}_model.pth')
            else:
                patience_counter += 1
            
            # æ‰“å°è¿›åº¦
            if epoch % 20 == 0 or patience_counter == 0:
                current_lr = optimizer.param_groups[0]['lr']
                print(f"   Epoch {epoch:3d}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, LR={current_lr:.6f}")
            
            # æ—©åœ
            if patience_counter >= patience:
                print(f"   â¹ï¸ æ—©åœè§¦å‘ (Epoch {epoch})")
                break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        model.load_state_dict(torch.load(self.output_dir / f'best_{model_name.lower()}_model.pth'))
        
        # æœ€ç»ˆé¢„æµ‹
        model.eval()
        with torch.no_grad():
            y_pred = model(X_test).squeeze().cpu().numpy()
        
        # è®¡ç®—æŒ‡æ ‡
        rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))
        mae = mean_absolute_error(self.y_test, y_pred)
        r2 = r2_score(self.y_test, y_pred)
        nasa_score = self.calculate_nasa_score(self.y_test, y_pred)
        phm_score = self.calculate_phm_score(self.y_test, y_pred)
        
        training_time = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        self.models[model_name] = model
        self.best_params[model_name] = model_kwargs
        self.results[model_name] = {
            'rmse': rmse,
            'mae': mae,
            'r2': r2,
            'nasa_score': nasa_score,
            'phm_score': phm_score,
            'training_time': training_time,
            'predictions': y_pred,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'best_params': model_kwargs
        }
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.training_history[model_name] = {
            'train_losses': train_losses,
            'val_losses': val_losses
        }
        
        print(f"âœ… {model_name}è®­ç»ƒå®Œæˆ!")
        print(f"   ğŸ“Š RMSE: {rmse:.3f}, MAE: {mae:.3f}, RÂ²: {r2:.4f}")
        print(f"   ğŸ¯ NASA Score: {nasa_score:.3f}, PHM Score: {phm_score:.4f}")
        print(f"   â±ï¸ è®­ç»ƒæ—¶é—´: {training_time:.1f}ç§’")
        
        return model, y_pred
    
    def train_gru(self):
        """è®­ç»ƒGRUæ¨¡å‹"""
        self.train_pytorch_model(
            self.OptimizedGRUModel, 'GRU',
            hidden_size=64, num_layers=2, dropout=0.3,
            use_batch_norm=True, use_residual=False,
            lr=0.001, batch_size=32, epochs=100, weight_decay=0.01
        )
    
    def train_bilstm(self):
        """è®­ç»ƒBiLSTMæ¨¡å‹"""
        self.train_pytorch_model(
            self.OptimizedBiLSTMModel, 'BiLSTM',
            hidden_size=64, num_layers=2, dropout=0.3,
            use_batch_norm=True, use_attention=True,
            lr=0.001, batch_size=32, epochs=100, weight_decay=0.01
        )
    
    def train_transformer(self):
        """è®­ç»ƒTransformeræ¨¡å‹"""
        self.train_pytorch_model(
            self.OptimizedTransformerModel, 'Transformer',
            d_model=128, nhead=8, num_layers=4, dropout=0.3,
            use_positional_encoding=True, use_layer_norm=True,
            lr=0.001, batch_size=32, epochs=100, weight_decay=0.01
        )
    
    def train_full_tft(self):
        """è®­ç»ƒFull TFTæ¨¡å‹"""
        self.train_pytorch_model(
            self.FullTemporalFusionTransformer, 'Full_TFT',
            hidden_size=128, num_heads=8, num_layers=3, dropout=0.3,
            lr=0.0005, batch_size=32, epochs=120, weight_decay=0.01
        )
    
    # ========== è®­ç»ƒæ§åˆ¶æ¨¡å— ==========
    def train_all_models(self):
        """è®­ç»ƒæ‰€æœ‰ä¸ƒä¸ªç®—æ³•"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒæ‰€æœ‰ä¸ƒä¸ªç®—æ³•...")
        print("="*80)
        
        # 1. ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•
        print(f"\nğŸ“Š è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ ç®—æ³•...")
        self.train_svr()
        self.train_xgboost()
        self.train_lightgbm()
        
        # 2. æ·±åº¦å­¦ä¹ ç®—æ³•
        if PYTORCH_AVAILABLE:
            print(f"\nğŸ§  è®­ç»ƒæ·±åº¦å­¦ä¹ ç®—æ³•...")
            self.train_gru()
            self.train_bilstm()
            self.train_transformer()
            self.train_full_tft()
        else:
            print("âŒ PyTorchä¸å¯ç”¨ï¼Œè·³è¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹")
        
        print(f"\nğŸ‰ æ‰€æœ‰ç®—æ³•è®­ç»ƒå®Œæˆï¼å…±è®­ç»ƒäº† {len(self.models)} ä¸ªæ¨¡å‹")
    
    # ========== å¯è§†åŒ–æ¨¡å— ==========
    def plot_individual_predictions(self):
        """ä¸ºæ¯ä¸ªç®—æ³•ç»˜åˆ¶å•ç‹¬çš„é¢„æµ‹RULå’ŒçœŸå®RULå¯¹æ¯”å›¾"""
        print(f"ğŸ“Š ç”Ÿæˆå„ç®—æ³•å•ç‹¬é¢„æµ‹å¯¹æ¯”å›¾...")
        
        if not self.results:
            print("âŒ æ²¡æœ‰ç»“æœå¯ä»¥ç»˜åˆ¶")
            return
        
        # ä¸ºæ¯ä¸ªç®—æ³•åˆ›å»ºå•ç‹¬çš„å›¾
        for model_name in self.results.keys():
            y_pred = self.results[model_name]['predictions']
            
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'{model_name} ç®—æ³•é¢„æµ‹ç»“æœåˆ†æ - {self.dataset}', fontsize=16, fontweight='bold')
            
            # 1. é¢„æµ‹vsçœŸå®å€¼æ•£ç‚¹å›¾
            ax1 = axes[0, 0]
            ax1.scatter(self.y_test, y_pred, alpha=0.6, s=50, color='blue')
            ax1.plot([0, self.max_rul], [0, self.max_rul], 'r--', lw=2, label='Perfect Prediction')
            ax1.set_xlabel('çœŸå®RUL', fontsize=12)
            ax1.set_ylabel('é¢„æµ‹RUL', fontsize=12)
            ax1.set_title('é¢„æµ‹å€¼ vs çœŸå®å€¼', fontsize=14, fontweight='bold')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # æ·»åŠ æ€§èƒ½æŒ‡æ ‡æ–‡æœ¬
            rmse = self.results[model_name]['rmse']
            r2 = self.results[model_name]['r2']
            phm = self.results[model_name]['phm_score']
            ax1.text(0.05, 0.95, f'RMSE: {rmse:.3f}\nRÂ²: {r2:.4f}\nPHM: {phm:.3f}', 
                    transform=ax1.transAxes, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
            
            # 2. æ—¶é—´åºåˆ—é¢„æµ‹å¯¹æ¯”å›¾
            ax2 = axes[0, 1]
            sample_indices = range(len(self.y_test))
            ax2.plot(sample_indices, self.y_test, 'b-', label='True RUL', linewidth=2, alpha=0.8)
            ax2.plot(sample_indices, y_pred, 'r-', label='Predicted RUL', linewidth=2, alpha=0.8)
            ax2.set_xlabel('Sample Index', fontsize=12)
            ax2.set_ylabel('RUL', fontsize=12)
            ax2.set_title('True vs Predicted RUL Time Series', fontsize=14, fontweight='bold')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            # 3. æ®‹å·®åˆ†å¸ƒå›¾
            ax3 = axes[1, 0]
            residuals = y_pred - self.y_test
            ax3.scatter(self.y_test, residuals, alpha=0.6, s=50, color='green')
            ax3.axhline(y=0, color='r', linestyle='--', lw=2)
            ax3.set_xlabel('çœŸå®RUL', fontsize=12)
            ax3.set_ylabel('æ®‹å·® (é¢„æµ‹å€¼ - çœŸå®å€¼)', fontsize=12)
            ax3.set_title('æ®‹å·®åˆ†å¸ƒå›¾', fontsize=14, fontweight='bold')
            ax3.grid(True, alpha=0.3)
            
            # 4. è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
            ax4 = axes[1, 1]
            ax4.hist(residuals, bins=30, alpha=0.7, color='orange', edgecolor='black')
            ax4.axvline(x=0, color='r', linestyle='--', lw=2, label='Zero Error')
            ax4.set_xlabel('æ®‹å·®', fontsize=12)
            ax4.set_ylabel('é¢‘æ¬¡', fontsize=12)
            ax4.set_title('æ®‹å·®åˆ†å¸ƒç›´æ–¹å›¾', fontsize=14, fontweight='bold')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(self.output_dir / f'{model_name}_individual_prediction_analysis.png', 
                       dpi=300, bbox_inches='tight')
            plt.show()
        
        print(f"âœ… å„ç®—æ³•å•ç‹¬é¢„æµ‹å¯¹æ¯”å›¾å·²ä¿å­˜åˆ° {self.output_dir}")
    
    def plot_training_curves(self):
        """ç»˜åˆ¶å„æ·±åº¦å­¦ä¹ ç®—æ³•çš„è®­ç»ƒæ›²çº¿"""
        print(f"ğŸ“Š ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾...")
        
        if not self.training_history:
            print("âŒ æ²¡æœ‰è®­ç»ƒå†å²æ•°æ®")
            return
        
        # æ·±åº¦å­¦ä¹ æ¨¡å‹
        dl_models = [name for name in self.training_history.keys() if name in ['GRU', 'BiLSTM', 'Transformer', 'Full_TFT']]
        
        if not dl_models:
            print("âŒ æ²¡æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒå†å²")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f'æ·±åº¦å­¦ä¹ ç®—æ³•è®­ç»ƒæ›²çº¿ - {self.dataset}', fontsize=16, fontweight='bold')
        
        colors = ['blue', 'red', 'green', 'orange']
        
        for i, model_name in enumerate(dl_models[:4]):
            row = i // 2
            col = i % 2
            ax = axes[row, col]
            
            history = self.training_history[model_name]
            train_losses = history['train_losses']
            val_losses = history['val_losses']
            epochs = range(1, len(train_losses) + 1)
            
            ax.plot(epochs, train_losses, label='Training Loss', color=colors[i], linewidth=2, alpha=0.8)
            ax.plot(epochs, val_losses, label='Validation Loss', color=colors[i], linewidth=2, linestyle='--', alpha=0.8)
            
            ax.set_xlabel('Epoch', fontsize=12)
            ax.set_ylabel('Loss', fontsize=12)
            ax.set_title(f'{model_name} è®­ç»ƒæ›²çº¿', fontsize=14, fontweight='bold')
            ax.legend()
            ax.grid(True, alpha=0.3)
            
            # æ ‡è®°æœ€ä½³epoch
            best_epoch = np.argmin(val_losses) + 1
            best_val_loss = min(val_losses)
            ax.scatter(best_epoch, best_val_loss, color='red', s=100, marker='*', zorder=5)
            ax.text(best_epoch, best_val_loss, f'Best: {best_val_loss:.4f}', 
                   ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / f'training_curves_{self.dataset}.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜åˆ° {self.output_dir}")
    
    def plot_metrics_comparison(self):
        """ç»˜åˆ¶å„ç®—æ³•å„æŒ‡æ ‡çš„å¯¹æ¯”å›¾"""
        print(f"ğŸ“Š ç”ŸæˆæŒ‡æ ‡å¯¹æ¯”å›¾...")
        
        if not self.results:
            print("âŒ æ²¡æœ‰ç»“æœå¯ä»¥ç»˜åˆ¶")
            return
        
        # åˆ›å»ºç»“æœDataFrame
        df_results = pd.DataFrame(self.results).T
        
        # è®¾ç½®å›¾å½¢
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle(f'ä¸ƒç®—æ³•æ€§èƒ½æŒ‡æ ‡å¯¹æ¯” - {self.dataset}', fontsize=16, fontweight='bold')
        
        metrics = ['rmse', 'mae', 'r2', 'nasa_score', 'phm_score', 'training_time']
        metric_names = ['RMSE', 'MAE', 'RÂ²', 'NASA Score', 'PHM Score', 'Training Time (s)']
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD']
        
        for i, (metric, name) in enumerate(zip(metrics, metric_names)):
            row = i // 3
            col = i % 3
            ax = axes[row, col]
            
            values = df_results[metric].sort_values(ascending=(metric not in ['r2']))
            bars = ax.bar(range(len(values)), values, color=colors[i], alpha=0.8)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for j, (bar, val) in enumerate(zip(bars, values)):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + max(values) * 0.01,
                       f'{val:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
            
            ax.set_xticks(range(len(values)))
            ax.set_xticklabels(values.index, rotation=45, ha='right')
            ax.set_ylabel(name, fontsize=12)
            ax.set_title(f'{name} å¯¹æ¯”', fontsize=14, fontweight='bold')
            ax.grid(axis='y', alpha=0.3)
            
            # é«˜äº®æœ€ä½³å€¼
            if metric == 'r2':
                best_idx = len(values) - 1  # RÂ²è¶Šå¤§è¶Šå¥½
            else:
                best_idx = 0  # å…¶ä»–æŒ‡æ ‡è¶Šå°è¶Šå¥½ï¼ˆé™¤äº†training_timeæ˜¯ä¿¡æ¯æ€§çš„ï¼‰
            
            if metric != 'training_time':  # è®­ç»ƒæ—¶é—´ä¸éœ€è¦é«˜äº®æœ€ä½³
                bars[best_idx].set_color('red')
                bars[best_idx].set_alpha(1.0)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / f'metrics_comparison_{self.dataset}.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜åˆ° {self.output_dir}")
    
    def plot_comprehensive_analysis(self):
        """ç»˜åˆ¶ç»¼åˆåˆ†æå›¾"""
        print(f"ğŸ“Š ç”Ÿæˆç»¼åˆåˆ†æå›¾...")
        
        if not self.results:
            print("âŒ æ²¡æœ‰ç»“æœå¯ä»¥ç»˜åˆ¶")
            return
        
        fig = plt.figure(figsize=(20, 16))
        fig.suptitle(f'ä¸ƒç®—æ³•RULé¢„æµ‹ç³»ç»Ÿç»¼åˆåˆ†æ - {self.dataset}', fontsize=18, fontweight='bold')
        
        # 1. ç®—æ³•æ€§èƒ½é›·è¾¾å›¾
        ax1 = plt.subplot(3, 3, 1, projection='polar')
        
        # å‡†å¤‡é›·è¾¾å›¾æ•°æ®
        algorithms = list(self.results.keys())
        metrics = ['rmse', 'mae', 'phm_score']  # é€‰æ‹©å…³é”®æŒ‡æ ‡
        
        # æ ‡å‡†åŒ–æŒ‡æ ‡ï¼ˆè¶Šå°è¶Šå¥½çš„æŒ‡æ ‡éœ€è¦åè½¬ï¼‰
        df_results = pd.DataFrame(self.results).T
        normalized_data = {}
        
        for metric in metrics:
            values = df_results[metric].values
            # æ ‡å‡†åŒ–åˆ°0-1ï¼Œè¶Šå°è¶Šå¥½çš„æŒ‡æ ‡åè½¬
            normalized = 1 - (values - values.min()) / (values.max() - values.min() + 1e-8)
            normalized_data[metric] = normalized
        
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆ
        
        colors = plt.cm.Set3(np.linspace(0, 1, len(algorithms)))
        
        for i, alg in enumerate(algorithms):
            values = [normalized_data[metric][i] for metric in metrics]
            values += values[:1]  # é—­åˆ
            
            ax1.plot(angles, values, 'o-', linewidth=2, label=alg, color=colors[i])
            ax1.fill(angles, values, alpha=0.25, color=colors[i])
        
        ax1.set_xticks(angles[:-1])
        ax1.set_xticklabels(['RMSE', 'MAE', 'PHM Score'])
        ax1.set_ylim(0, 1)
        ax1.set_title('ç®—æ³•æ€§èƒ½é›·è¾¾å›¾', fontsize=14, fontweight='bold', pad=20)
        ax1.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        
        # 2. æœ€ä½³ç®—æ³•é¢„æµ‹æ•£ç‚¹å›¾
        ax2 = plt.subplot(3, 3, 2)
        best_alg = min(self.results.keys(), key=lambda x: self.results[x]['phm_score'])
        best_pred = self.results[best_alg]['predictions']
        
        ax2.scatter(self.y_test, best_pred, alpha=0.6, s=50, color='blue')
        ax2.plot([0, self.max_rul], [0, self.max_rul], 'r--', lw=2)
        ax2.set_xlabel('çœŸå®RUL')
        ax2.set_ylabel('é¢„æµ‹RUL')
        ax2.set_title(f'æœ€ä½³ç®—æ³•é¢„æµ‹æ•ˆæœ ({best_alg})')
        ax2.grid(True, alpha=0.3)
        
        # 3. ç®—æ³•æ’åæ¡å½¢å›¾
        ax3 = plt.subplot(3, 3, 3)
        phm_scores = df_results['phm_score'].sort_values()
        bars = ax3.barh(range(len(phm_scores)), phm_scores.values, color='lightcoral')
        ax3.set_yticks(range(len(phm_scores)))
        ax3.set_yticklabels(phm_scores.index)
        ax3.set_xlabel('PHM Score')
        ax3.set_title('ç®—æ³•PHM Scoreæ’å')
        ax3.grid(axis='x', alpha=0.3)
        
        # æ ‡è®°æœ€ä½³
        bars[0].set_color('red')
        
        # 4-6. å„æŒ‡æ ‡è¯¦ç»†å¯¹æ¯”
        metrics_detail = ['rmse', 'r2', 'training_time']
        metric_names_detail = ['RMSE', 'RÂ² Score', 'Training Time (s)']
        
        for i, (metric, name) in enumerate(zip(metrics_detail, metric_names_detail)):
            ax = plt.subplot(3, 3, 4 + i)
            values = df_results[metric].sort_values(ascending=(metric != 'r2'))
            bars = ax.bar(range(len(values)), values, alpha=0.8)
            ax.set_xticks(range(len(values)))
            ax.set_xticklabels(values.index, rotation=45, ha='right')
            ax.set_ylabel(name)
            ax.set_title(f'{name} å¯¹æ¯”')
            ax.grid(axis='y', alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, val in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{val:.3f}', ha='center', va='bottom', fontsize=9)
        
        # 7. é¢„æµ‹è¯¯å·®åˆ†å¸ƒ
        ax7 = plt.subplot(3, 3, 7)
        for i, alg in enumerate(algorithms):
            pred = self.results[alg]['predictions']
            errors = np.abs(pred - self.y_test)
            ax7.hist(errors, bins=20, alpha=0.5, label=alg, density=True)
        
        ax7.set_xlabel('ç»å¯¹è¯¯å·®')
        ax7.set_ylabel('å¯†åº¦')
        ax7.set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ')
        ax7.legend()
        ax7.grid(True, alpha=0.3)
        
        # 8. ç®—æ³•å¤æ‚åº¦vsæ€§èƒ½
        ax8 = plt.subplot(3, 3, 8)
        training_times = [self.results[alg]['training_time'] for alg in algorithms]
        phm_scores_list = [self.results[alg]['phm_score'] for alg in algorithms]
        
        scatter = ax8.scatter(training_times, phm_scores_list, s=100, alpha=0.7, c=range(len(algorithms)), cmap='viridis')
        
        for i, alg in enumerate(algorithms):
            ax8.annotate(alg, (training_times[i], phm_scores_list[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=9)
        
        ax8.set_xlabel('è®­ç»ƒæ—¶é—´ (ç§’)')
        ax8.set_ylabel('PHM Score')
        ax8.set_title('ç®—æ³•å¤æ‚åº¦ vs æ€§èƒ½')
        ax8.grid(True, alpha=0.3)
        
        # 9. æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        ax9 = plt.subplot(3, 3, 9)
        ax9.axis('off')
        
        stats_text = f"""
æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:
â€¢ æ•°æ®é›†: {self.dataset}
â€¢ è®­ç»ƒæ ·æœ¬: {len(self.y_train)}
â€¢ æµ‹è¯•æ ·æœ¬: {len(self.y_test)}
â€¢ ç‰¹å¾ç»´åº¦: {len(self.feature_cols)}
â€¢ çª—å£å¤§å°: {self.window_size}
â€¢ æœ€å¤§RUL: {self.max_rul}

æœ€ä½³ç®—æ³•: {best_alg}
â€¢ RMSE: {self.results[best_alg]['rmse']:.3f}
â€¢ RÂ²: {self.results[best_alg]['r2']:.4f}
â€¢ PHM Score: {self.results[best_alg]['phm_score']:.3f}
        """
        
        ax9.text(0.1, 0.9, stats_text, transform=ax9.transAxes, fontsize=11,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(self.output_dir / f'comprehensive_analysis_{self.dataset}.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… ç»¼åˆåˆ†æå›¾å·²ä¿å­˜åˆ° {self.output_dir}")
    
    # ========== æ•°æ®ä¿å­˜æ¨¡å— ==========
    def save_results_and_data(self):
        """ä¿å­˜ç»“æœå’Œæ•°æ®"""
        print(f"ğŸ’¾ ä¿å­˜ç»“æœå’Œæ•°æ®...")
        
        # 1. ä¿å­˜è¯„ä¼°ç»“æœ
        if self.results:
            df_results = pd.DataFrame(self.results).T
            df_results.to_csv(self.output_dir / f'results_{self.dataset}.csv')
            print(f"   âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ° results_{self.dataset}.csv")
        
        # 2. ä¿å­˜æœ€ä½³å‚æ•°
        if self.best_params:
            with open(self.output_dir / f'best_params_{self.dataset}.json', 'w', encoding='utf-8') as f:
                json.dump(self.best_params, f, indent=2, ensure_ascii=False, default=str)
            print(f"   âœ… æœ€ä½³å‚æ•°å·²ä¿å­˜åˆ° best_params_{self.dataset}.json")
        
        # 3. ä¿å­˜è®­ç»ƒå†å²
        if self.training_history:
            with open(self.output_dir / f'training_history_{self.dataset}.json', 'w', encoding='utf-8') as f:
                json.dump(self.training_history, f, indent=2, ensure_ascii=False, default=str)
            print(f"   âœ… è®­ç»ƒå†å²å·²ä¿å­˜åˆ° training_history_{self.dataset}.json")
        
        # 4. ä¿å­˜é¢„æµ‹ç»“æœ
        predictions_data = {}
        predictions_data['y_test'] = self.y_test.tolist()
        for model_name in self.results.keys():
            predictions_data[f'{model_name}_predictions'] = self.results[model_name]['predictions'].tolist()
        
        with open(self.output_dir / f'predictions_{self.dataset}.json', 'w', encoding='utf-8') as f:
            json.dump(predictions_data, f, indent=2, ensure_ascii=False)
        print(f"   âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° predictions_{self.dataset}.json")
        
        # 5. ä¿å­˜æ¨¡å‹ï¼ˆä¼ ç»ŸMLæ¨¡å‹ï¼‰
        for model_name, model in self.models.items():
            if model_name in ['SVR', 'XGBoost', 'LightGBM']:
                with open(self.output_dir / f'{model_name}_model_{self.dataset}.pkl', 'wb') as f:
                    pickle.dump(model, f)
                print(f"   âœ… {model_name}æ¨¡å‹å·²ä¿å­˜åˆ° {model_name}_model_{self.dataset}.pkl")
        
        print(f"ğŸ’¾ æ‰€æœ‰ç»“æœå’Œæ•°æ®å·²ä¿å­˜åˆ° {self.output_dir}")
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        if not self.results:
            print("âŒ æ²¡æœ‰ç»“æœå¯ä»¥ç”ŸæˆæŠ¥å‘Š")
            return
        
        print(f"\n" + "="*100)
        print(f"ğŸ¯ å®Œæ•´ä¸ƒç®—æ³•RULé¢„æµ‹ç³»ç»Ÿ - æ€§èƒ½æŠ¥å‘Š")
        print(f"="*100)
        print(f"ğŸ“Š æ•°æ®é›†: {self.dataset}")
        print(f"ğŸ”§ çª—å£å¤§å°: {self.window_size}")
        print(f"ğŸ¯ æœ€å¤§RUL: {self.max_rul}")
        print(f"ğŸ’» è®¾å¤‡: {self.device}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # æŒ‰PHM Scoreæ’åº
        sorted_results = sorted(self.results.items(), key=lambda x: x[1]['phm_score'])
        
        print(f"\nğŸ† ç®—æ³•æ€§èƒ½æ’è¡Œæ¦œ (æŒ‰PHM Scoreæ’åº):")
        print("-"*100)
        print(f"{'æ’å':<4} {'ç®—æ³•':<12} {'RMSE':<8} {'MAE':<8} {'RÂ²':<8} {'NASA':<10} {'PHM':<8} {'æ—¶é—´':<8}")
        print("-"*100)
        
        for i, (name, result) in enumerate(sorted_results, 1):
            print(f"{i:<4} {name:<12} {result['rmse']:<8.3f} {result['mae']:<8.3f} "
                  f"{result['r2']:<8.4f} {result['nasa_score']:<10.2f} {result['phm_score']:<8.3f} {result['training_time']:<8.1f}s")
        
        # æœ€ä½³ç®—æ³•è¯¦ç»†ä¿¡æ¯
        best_name, best_result = sorted_results[0]
        print(f"\nğŸ¥‡ æœ€ä½³ç®—æ³•: {best_name}")
        print(f"   ğŸ“Š RMSE: {best_result['rmse']:.3f}")
        print(f"   ğŸ“Š MAE: {best_result['mae']:.3f}")
        print(f"   ğŸ“Š RÂ²: {best_result['r2']:.4f}")
        print(f"   ğŸ¯ NASA Score: {best_result['nasa_score']:.3f}")
        print(f"   ğŸ† PHM Score: {best_result['phm_score']:.3f}")
        print(f"   â±ï¸ è®­ç»ƒæ—¶é—´: {best_result['training_time']:.1f}ç§’")
        
        # ç®—æ³•ç±»å‹åˆ†æ
        ml_algorithms = ['SVR', 'XGBoost', 'LightGBM']
        dl_algorithms = ['GRU', 'BiLSTM', 'Transformer', 'Full_TFT']
        
        ml_results = {name: result for name, result in self.results.items() if name in ml_algorithms}
        dl_results = {name: result for name, result in self.results.items() if name in dl_algorithms}
        
        if ml_results:
            best_ml = min(ml_results.items(), key=lambda x: x[1]['phm_score'])
            print(f"\nğŸ¤– æœ€ä½³ä¼ ç»ŸMLç®—æ³•: {best_ml[0]} (PHM: {best_ml[1]['phm_score']:.3f})")
        
        if dl_results:
            best_dl = min(dl_results.items(), key=lambda x: x[1]['phm_score'])
            print(f"ğŸ§  æœ€ä½³æ·±åº¦å­¦ä¹ ç®—æ³•: {best_dl[0]} (PHM: {best_dl[1]['phm_score']:.3f})")
        
        # æ€§èƒ½ç»Ÿè®¡
        all_phm_scores = [result['phm_score'] for result in self.results.values()]
        all_rmse_scores = [result['rmse'] for result in self.results.values()]
        
        print(f"\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
        print(f"   PHM Score - æœ€ä½³: {min(all_phm_scores):.3f}, æœ€å·®: {max(all_phm_scores):.3f}, å¹³å‡: {np.mean(all_phm_scores):.3f}")
        print(f"   RMSE - æœ€ä½³: {min(all_rmse_scores):.3f}, æœ€å·®: {max(all_rmse_scores):.3f}, å¹³å‡: {np.mean(all_rmse_scores):.3f}")
        
        print(f"\nğŸ’¾ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
        print(f"="*100)
    
    # ========== ä¸»æ§åˆ¶æ¨¡å— ==========
    def run_complete_analysis(self):
        """è¿è¡Œå®Œæ•´çš„ä¸ƒç®—æ³•åˆ†æ"""
        print(f"ğŸš€ å¼€å§‹å®Œæ•´ä¸ƒç®—æ³•RULé¢„æµ‹ç³»ç»Ÿåˆ†æ")
        print(f"="*80)
        
        try:
            # 1. æ•°æ®å¤„ç†
            print(f"\nğŸ“Š ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®å¤„ç†")
            self.load_data()
            self.preprocess_data()
            self.feature_engineering()
            
            # 2. æ¨¡å‹è®­ç»ƒ
            print(f"\nğŸ¤– ç¬¬äºŒé˜¶æ®µï¼šæ¨¡å‹è®­ç»ƒ")
            self.train_all_models()
            
            # 3. ç»“æœå¯è§†åŒ–
            print(f"\nğŸ“Š ç¬¬ä¸‰é˜¶æ®µï¼šç»“æœå¯è§†åŒ–")
            self.plot_individual_predictions()
            self.plot_training_curves()
            self.plot_metrics_comparison()
            self.plot_comprehensive_analysis()
            
            # 4. æ•°æ®ä¿å­˜
            print(f"\nğŸ’¾ ç¬¬å››é˜¶æ®µï¼šæ•°æ®ä¿å­˜")
            self.save_results_and_data()
            
            # 5. ç”ŸæˆæŠ¥å‘Š
            print(f"\nğŸ“‹ ç¬¬äº”é˜¶æ®µï¼šç”ŸæˆæŠ¥å‘Š")
            self.generate_summary_report()
            
            print(f"\nğŸ‰ å®Œæ•´ä¸ƒç®—æ³•RULé¢„æµ‹ç³»ç»Ÿåˆ†æå®Œæˆ!")
            
        except Exception as e:
            print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å®Œæ•´ä¸ƒç®—æ³•RULé¢„æµ‹ç³»ç»Ÿ")
    print("ğŸ¤– ç®—æ³•: SVR, GRU, BiLSTM, Transformer, Full TFT, XGBoost, LightGBM")
    print("ğŸ”§ ç‰¹è‰²: å®Œæ•´æ•°æ®å¤„ç†, å‚æ•°ä¼˜åŒ–, è¯¦ç»†å¯è§†åŒ–, æ¨¡å—åŒ–è®¾è®¡")
    print("="*80)
    
    # å¯ä»¥é€‰æ‹©è¿è¡Œå•ä¸ªæ•°æ®é›†æˆ–å¤šä¸ªæ•°æ®é›†
    datasets = ['FD001']  # å¯ä»¥æ‰©å±•ä¸º ['FD001', 'FD002', 'FD003', 'FD004']
    
    for dataset in datasets:
        print(f"\n{'='*80}")
        print(f"å¤„ç†æ•°æ®é›†: {dataset}")
        print(f"{'='*80}")
        
        try:
            # åˆ›å»ºç³»ç»Ÿå®ä¾‹
            system = CompleteSevenAlgorithmsRULSystem(
                dataset=dataset, 
                window_size=30, 
                max_rul=125, 
                device='cuda'
            )
            
            # è¿è¡Œå®Œæ•´åˆ†æ
            system.run_complete_analysis()
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ•°æ®é›† {dataset} æ—¶å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\nğŸ‰ æ‰€æœ‰æ•°æ®é›†å¤„ç†å®Œæˆ!")

if __name__ == "__main__":
    main()